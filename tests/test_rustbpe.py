"""
æ¯”è¾ƒä»¥ä¸‹åˆ†è¯å™¨çš„è®­ç»ƒï¼š

1. ï¼ˆéå¸¸æ…¢ï¼‰Pythonå‚è€ƒå®ç°
2. ä¼˜åŒ–çš„Pythonå®ç°
3. HuggingFaceåˆ†è¯å™¨è®­ç»ƒå®ç°
4. æˆ‘ä»¬è‡ªå·±çš„è‡ªå®šä¹‰RustBPEè®­ç»ƒå®ç°

æ‰€æœ‰è¿™äº›éƒ½åº”è¯¥è®¡ç®—ç›¸åŒçš„åˆå¹¶å¹¶äº§ç”Ÿ
ç›¸åŒçš„è¯æ±‡è¡¨å’Œåˆ†è¯ç»“æœã€‚

æœ€åï¼Œä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨tiktokenè¿›è¡Œæ¨ç†ã€‚
å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›ç¡®ä¿å¯ä»¥å°†æˆ‘ä»¬çš„rustbpeåˆ†è¯å™¨
å¯¼å‡ºåˆ°tiktokenï¼Œå¹¶åœ¨æ¨ç†ä¸­ä½¿ç”¨ç›¸åŒçš„ç»“æœã€‚

è¿è¡Œæ–¹å¼ï¼š
python -m pytest tests/test_rustbpe.py -v -s
-v æ˜¯è¯¦ç»†æ¨¡å¼ï¼Œ-s æ˜¯æ˜¾ç¤ºæ‰“å°
"""

import regex as re
from collections import Counter, defaultdict
import time
import rustbpe
import tiktoken
import pytest

GPT4_SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

# -----------------------------------------------------------------------------
# Reference tokenizer, pretty much copy pasted and pruned a bit from minbpe

def get_stats(ids, counts=None):
    """
    ç»™å®šä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼Œè¿”å›è¿ç»­å¯¹çš„è®¡æ•°å­—å…¸
    ç¤ºä¾‹ï¼š[1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    å¯é€‰åœ°å…è®¸æ›´æ–°ç°æœ‰çš„è®¡æ•°å­—å…¸
    """
    counts = {} if counts is None else counts
    for pair in zip(ids, ids[1:]): # iterate consecutive elements
        counts[pair] = counts.get(pair, 0) + 1
    return counts

def merge(ids, pair, idx):
    """
    åœ¨æ•´æ•°åˆ—è¡¨ï¼ˆidsï¼‰ä¸­ï¼Œå°†æ‰€æœ‰è¿ç»­å‡ºç°çš„
    å¯¹æ›¿æ¢ä¸ºæ–°çš„æ•´æ•°token idx
    ç¤ºä¾‹ï¼šids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]
    """
    newids = []
    i = 0
    while i < len(ids):
        # if not at the very last position AND the pair matches, replace it
        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:
            newids.append(idx)
            i += 2
        else:
            newids.append(ids[i])
            i += 1
    return newids

class RegexTokenizer:
    """åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„åˆ†è¯å™¨å‚è€ƒå®ç°"""

    def __init__(self, pattern=None):
        """
        - pattern: å¯é€‰å­—ç¬¦ä¸²ï¼Œç”¨äºè¦†ç›–é»˜è®¤å€¼ï¼ˆGPT-4åˆ†å‰²æ¨¡å¼ï¼‰
        - special_tokens: str -> int ç‰¹æ®Štokenå­—å…¸
          ç¤ºä¾‹ï¼š{'<|endoftext|>': 100257}
        """
        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern
        self.merges = {} # (int, int) -> int
        self.compiled_pattern = re.compile(self.pattern)
        self.special_tokens = {}
        self.inverse_special_tokens = {}
        self.vocab = self._build_vocab()

    def _build_vocab(self):
        """è¯æ±‡è¡¨ç®€å•ä¸”ç¡®å®šæ€§åœ°ä»åˆå¹¶ä¸­æ´¾ç”Ÿ"""
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        for special, idx in self.special_tokens.items():
            vocab[idx] = special.encode("utf-8")
        return vocab

    def train(self, text, vocab_size, verbose=False):
        """è®­ç»ƒåˆ†è¯å™¨"""
        assert vocab_size >= 256
        num_merges = vocab_size - 256

        # è·Ÿè¸ªè®­ç»ƒæœŸé—´åˆå¹¶æ˜¯å¦åœ¨ä»»ä½•æ—¶å€™æ˜¯æ¨¡ç³Šçš„ï¼ˆå¯¹çš„è®¡æ•°ä¸å”¯ä¸€ï¼‰
        ambiguous = False

        # å°†æ–‡æœ¬åˆ†å‰²æˆæ–‡æœ¬å—
        text_chunks = re.findall(self.compiled_pattern, text)

        # è¾“å…¥æ–‡æœ¬é¢„å¤„ç†
        ids = [list(ch.encode("utf-8")) for ch in text_chunks]

        # è¿­ä»£åˆå¹¶æœ€å¸¸è§çš„å¯¹ä»¥åˆ›å»ºæ–°token
        merges = {} # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes
        for i in range(num_merges):
            # è®¡ç®—æ¯ä¸ªè¿ç»­å¯¹å‡ºç°çš„æ¬¡æ•°
            stats = {}
            for chunk_ids in ids:
                # ä¼ å…¥statså°†åœ¨åŸåœ°æ›´æ–°å®ƒï¼Œç´¯åŠ è®¡æ•°
                get_stats(chunk_ids, stats)
            # æ‰¾åˆ°è®¡æ•°æœ€é«˜çš„å¯¹
            pair = max(stats, key=stats.get)
            # æ£€æŸ¥åˆå¹¶æ˜¯å¦æ¨¡ç³Š - å³æœ€å¤§å€¼ä¸å”¯ä¸€
            pair_count = stats[pair]
            pairs_with_max_count = [pair for pair, count in stats.items() if count == pair_count]
            if len(pairs_with_max_count) > 1:
                # æ‰“å°å‰10å¯¹åŠå…¶è®¡æ•°
                # print(f"{i} Merge is ambiguous! {pair} has {pair_count} occurrences")
                # for print_pair, print_count in sorted(stats.items(), key=lambda x: x[1], reverse=True)[:10]:
                #     print(f"{print_pair}: {print_count}")
                ambiguous = True
            # åˆ›å»ºä¸€ä¸ªæ–°tokenï¼šåˆ†é…ä¸‹ä¸€ä¸ªå¯ç”¨çš„id
            idx = 256 + i
            # å°†idsä¸­æ‰€æœ‰å‡ºç°çš„å¯¹æ›¿æ¢ä¸ºidx
            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]
            # ä¿å­˜åˆå¹¶
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]
            # æ‰“å°
            if verbose:
                print(f"åˆå¹¶ {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) æœ‰ {stats[pair]} æ¬¡å‡ºç°")

        # ä¿å­˜ç±»å˜é‡
        self.merges = merges # åœ¨encode()ä¸­ä½¿ç”¨
        self.vocab = vocab   # åœ¨decode()ä¸­ä½¿ç”¨
        return ambiguous

    def _encode_chunk(self, text_bytes):
        """è¿”å›token id"""
        # å¼€å§‹ã€‚é¦–å…ˆï¼Œå°†æ‰€æœ‰å­—èŠ‚è½¬æ¢ä¸º0..255èŒƒå›´å†…çš„æ•´æ•°
        ids = list(text_bytes)
        while len(ids) >= 2:
            # æ‰¾åˆ°åˆå¹¶ç´¢å¼•æœ€ä½çš„å¯¹
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            # å¾®å¦™ä¹‹å¤„ï¼šå¦‚æœæ²¡æœ‰æ›´å¤šå¯ç”¨çš„åˆå¹¶ï¼Œé”®å°†ä¸º
            # æ¯ä¸ªå¯¹äº§ç”Ÿinfï¼Œè€Œminå°†æ˜¯
            # åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå¯¹ï¼Œä»»æ„åœ°
            # æˆ‘ä»¬å¯ä»¥é€šè¿‡æˆå‘˜èµ„æ ¼æ£€æŸ¥æ¥æ£€æµ‹è¿™ä¸ªç»ˆæ­¢æƒ…å†µ
            if pair not in self.merges:
                break # æ²¡æœ‰å…¶ä»–å¯ä»¥åˆå¹¶çš„äº†
            # å¦åˆ™è®©æˆ‘ä»¬åˆå¹¶æœ€ä½³å¯¹ï¼ˆæœ€ä½åˆå¹¶ç´¢å¼•ï¼‰
            idx = self.merges[pair]
            ids = merge(ids, pair, idx)
        return ids

    def encode_ordinary(self, text):
        """å¿½ç•¥ä»»ä½•ç‰¹æ®Štokençš„ç¼–ç ã€‚"""
        # æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼å®šä¹‰çš„ç±»åˆ«å°†æ–‡æœ¬åˆ†å‰²æˆæ–‡æœ¬å—
        text_chunks = re.findall(self.compiled_pattern, text)
        # æ‰€æœ‰æ–‡æœ¬å—åˆ†åˆ«ç¼–ç ï¼Œç„¶åç»“æœè¿æ¥
        ids = []
        for chunk in text_chunks:
            chunk_bytes = chunk.encode("utf-8") # åŸå§‹å­—èŠ‚
            chunk_ids = self._encode_chunk(chunk_bytes)
            ids.extend(chunk_ids)
        return ids

# -----------------------------------------------------------------------------
# Faster Python tokenizer, optimized version of the reference tokenizer

def fast_merge_inplace(ids, pair, idx):
    """
    åœ¨æ•´æ•°åˆ—è¡¨ï¼ˆidsï¼‰ä¸­ï¼Œå°†æ‰€æœ‰è¿ç»­å‡ºç°çš„
    å¯¹æ›¿æ¢ä¸ºæ–°çš„æ•´æ•°token idxï¼ˆåŸåœ°æ“ä½œï¼‰
    ç¤ºä¾‹ï¼šids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]
    """
    # Find all positions where the pair occurs
    i = 0
    while i < len(ids) - 1:
        if ids[i] == pair[0] and ids[i+1] == pair[1]:
            ids[i] = idx
            ids.pop(i+1)
        else:
            i += 1
    return ids


class FastRegexTokenizer:
    """ä¼˜åŒ–çš„åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„åˆ†è¯å™¨"""

    def __init__(self, pattern=None):
        """
        - pattern: å¯é€‰å­—ç¬¦ä¸²ï¼Œç”¨äºè¦†ç›–é»˜è®¤å€¼ï¼ˆGPT-4åˆ†å‰²æ¨¡å¼ï¼‰
        - special_tokens: str -> int ç‰¹æ®Štokenå­—å…¸
          ç¤ºä¾‹ï¼š{'<|endoftext|>': 100257}
        """
        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern
        self.compiled_pattern = re.compile(self.pattern)
        self.special_tokens = {}
        self.inverse_special_tokens = {}
        self.merges = {}
        self.vocab = self._build_vocab()

    def _build_vocab(self):
        """è¯æ±‡è¡¨ç®€å•ä¸”ç¡®å®šæ€§åœ°ä»åˆå¹¶ä¸­æ´¾ç”Ÿ"""
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        for special, idx in self.special_tokens.items():
            vocab[idx] = special.encode("utf-8")
        return vocab

    def train(self, text, vocab_size, verbose=False):
        """
        å¼•å…¥äº†è®¸å¤šä¼˜åŒ–ï¼š
        - é€šè¿‡å†…è”å‡½æ•°åˆ é™¤å‡½æ•°è°ƒç”¨å¼€é”€
        - ä½¿ç”¨.pop()åŸåœ°ä¿®æ”¹idsåˆ—è¡¨è€Œä¸æ˜¯åˆ›å»ºæ–°åˆ—è¡¨
        - å°†ç›¸åŒçš„å—æŠ˜å ä¸ºå”¯ä¸€çš„å—
        - æ›´èªæ˜åœ°æ›´æ–°è®¡æ•° - ä»…å›´ç»•å—å½±å“çš„å—
        """
        assert vocab_size >= 256
        num_merges = vocab_size - 256

        # split the text up into text chunks
        text_chunks = re.findall(self.compiled_pattern, text)

        # many, many chunks are identical, so we can "collapse" them to just the unique ones
        counts = Counter(text_chunks)
        unique_chunks = [ch for ch, count in counts.items()]
        chunk_counts = [count for ch, count in counts.items()]

        # input text preprocessing
        ids = [list(ch.encode("utf-8")) for ch in unique_chunks]
        # iteratively merge the most common pairs to create new tokens
        merges = {} # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes

        # Initial count: build stats and position tracking
        stats = defaultdict(int)
        positions = defaultdict(set)  # pair -> set of chunk indices that contain this pair

        for chunk_idx, (chunk_ids, count) in enumerate(zip(ids, chunk_counts)):
            for pair in zip(chunk_ids, chunk_ids[1:]):
                stats[pair] += count
                positions[pair].add(chunk_idx)

        for i in range(num_merges):
            if not stats:
                break

            # find the pair with the highest count
            pair = max(stats, key=stats.get)
            # mint a new token: assign it the next available id
            idx = 256 + i

            # Get chunks that contain this pair
            affected_chunks = positions[pair]

            # Track count changes for incremental update
            count_changes = defaultdict(int)

            # Replace all occurrences of pair in affected chunks only
            for chunk_idx in affected_chunks:
                chunk_ids = ids[chunk_idx]
                chunk_count = chunk_counts[chunk_idx]
                ix = 0
                while ix < len(chunk_ids) - 1:
                    if chunk_ids[ix] == pair[0] and chunk_ids[ix+1] == pair[1]:
                        # Track what pairs are being removed/added
                        # Remove: (prev, A), (A, B), (B, next)
                        if ix > 0:
                            old_left = (chunk_ids[ix-1], chunk_ids[ix])
                            count_changes[old_left] -= chunk_count

                        # The merged pair disappears
                        count_changes[pair] -= chunk_count

                        if ix + 2 < len(chunk_ids):
                            old_right = (chunk_ids[ix+1], chunk_ids[ix+2])
                            count_changes[old_right] -= chunk_count

                        # Apply the merge
                        chunk_ids[ix] = idx
                        chunk_ids.pop(ix+1)

                        # Add: (prev, C), (C, next)
                        if ix > 0:
                            new_left = (chunk_ids[ix-1], chunk_ids[ix])
                            count_changes[new_left] += chunk_count

                        if ix + 1 < len(chunk_ids):
                            new_right = (chunk_ids[ix], chunk_ids[ix+1])
                            count_changes[new_right] += chunk_count
                    else:
                        ix += 1

            # Apply incremental changes to stats and positions
            for changed_pair, delta in count_changes.items():
                if changed_pair == pair:
                    # The merged pair should disappear completely
                    continue

                stats[changed_pair] += delta

                # Update positions for changed pairs - only check affected chunks
                for chunk_idx in affected_chunks:
                    chunk_ids = ids[chunk_idx]
                    contains_pair = any((chunk_ids[j], chunk_ids[j+1]) == changed_pair
                                      for j in range(len(chunk_ids) - 1))
                    if contains_pair:
                        positions[changed_pair].add(chunk_idx)
                    else:
                        positions[changed_pair].discard(chunk_idx)

            # Remove the merged pair completely
            del stats[pair]
            del positions[pair]

            # save the merge
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]

        # save class variables
        self.merges = merges # used in encode()
        self.vocab = vocab   # used in decode()

    def register_special_tokens(self, special_tokens):
        """æ³¨å†Œç‰¹æ®Štoken"""
        # special_tokens æ˜¯ä¸€ä¸ª str -> int çš„å­—å…¸
        # ç¤ºä¾‹ï¼š{"<|endoftext|>": 100257}
        self.special_tokens = special_tokens
        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}

    def decode(self, ids):
        """ç»™å®šidsï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰ï¼Œè¿”å›Pythonå­—ç¬¦ä¸²"""
        part_bytes = []
        for idx in ids:
            if idx in self.vocab:
                part_bytes.append(self.vocab[idx])
            elif idx in self.inverse_special_tokens:
                part_bytes.append(self.inverse_special_tokens[idx].encode("utf-8"))
            else:
                raise ValueError(f"invalid token id: {idx}")
        text_bytes = b"".join(part_bytes)
        text = text_bytes.decode("utf-8", errors="replace")
        return text

    def _encode_chunk(self, text_bytes):
        """è¿”å›token id"""
        # å¼€å§‹ã€‚é¦–å…ˆï¼Œå°†æ‰€æœ‰å­—èŠ‚è½¬æ¢ä¸º0..255èŒƒå›´å†…çš„æ•´æ•°
        ids = list(text_bytes)
        while len(ids) >= 2:
            # æ‰¾åˆ°åˆå¹¶ç´¢å¼•æœ€ä½çš„å¯¹
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            # å¾®å¦™ä¹‹å¤„ï¼šå¦‚æœæ²¡æœ‰æ›´å¤šå¯ç”¨çš„åˆå¹¶ï¼Œé”®å°†ä¸º
            # æ¯ä¸ªå¯¹äº§ç”Ÿinfï¼Œè€Œminå°†æ˜¯
            # åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå¯¹ï¼Œä»»æ„åœ°
            # æˆ‘ä»¬å¯ä»¥é€šè¿‡æˆå‘˜èµ„æ ¼æ£€æŸ¥æ¥æ£€æµ‹è¿™ä¸ªç»ˆæ­¢æƒ…å†µ
            if pair not in self.merges:
                break # æ²¡æœ‰å…¶ä»–å¯ä»¥åˆå¹¶çš„äº†
            # å¦åˆ™è®©æˆ‘ä»¬åˆå¹¶æœ€ä½³å¯¹ï¼ˆæœ€ä½åˆå¹¶ç´¢å¼•ï¼‰
            idx = self.merges[pair]
            ids = fast_merge_inplace(ids, pair, idx)
        return ids

    def encode_ordinary(self, text):
        """å¿½ç•¥ä»»ä½•ç‰¹æ®Štokençš„ç¼–ç ã€‚"""
        # æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼å®šä¹‰çš„ç±»åˆ«å°†æ–‡æœ¬åˆ†å‰²æˆæ–‡æœ¬å—
        text_chunks = re.findall(self.compiled_pattern, text)
        # æ‰€æœ‰æ–‡æœ¬å—åˆ†åˆ«ç¼–ç ï¼Œç„¶åç»“æœè¿æ¥
        ids = []
        for chunk in text_chunks:
            chunk_bytes = chunk.encode("utf-8") # åŸå§‹å­—èŠ‚
            chunk_ids = self._encode_chunk(chunk_bytes)
            ids.extend(chunk_ids)
        return ids

# -----------------------------------------------------------------------------
# HuggingFace tokenizer
from tokenizers import Tokenizer as HFTokenizer
from tokenizers import pre_tokenizers, decoders, Regex
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer

class HuggingFaceTokenizer:
    """HuggingFaceåˆ†è¯å™¨çš„è½»é‡åŒ…è£…å™¨ï¼Œç”¨äºä¸€äº›å®ç”¨åŠŸèƒ½"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    @classmethod
    def train_from_iterator(cls, text_iterator, vocab_size):
        """ä»æ–‡æœ¬è¿­ä»£å™¨è®­ç»ƒ"""
        # é…ç½®HuggingFaceåˆ†è¯å™¨
        tokenizer = HFTokenizer(BPE(
            byte_fallback=True, # éœ€è¦ï¼
            unk_token=None,
            fuse_unk=False,
        ))
        # æ ‡å‡†åŒ–å™¨ï¼šæ— 
        tokenizer.normalizer = None
        # é¢„åˆ†è¯å™¨ï¼šGPT-4é£æ ¼
        gpt4_split_regex = Regex(GPT4_SPLIT_PATTERN) # huggingfaceè¦æ±‚ä½ å°†å…¶åŒ…è£…åœ¨Regexä¸­ï¼ï¼
        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
            pre_tokenizers.Split(pattern=gpt4_split_regex, behavior="isolated", invert=False),
            pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)
        ])
        # è§£ç å™¨ï¼šByteLevelï¼ˆå®ƒä¸ByteLevelé¢„åˆ†è¯å™¨é…å¯¹ï¼‰
        tokenizer.decoder = decoders.ByteLevel()
        # åå¤„ç†å™¨ï¼šæ— 
        tokenizer.post_processor = None
        # è®­ç»ƒå™¨ï¼šBPE
        trainer = BpeTrainer(
            vocab_size=vocab_size,
            show_progress=True,
            min_frequency=0, # æ— æœ€å°é¢‘ç‡
            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
            special_tokens=[], # æ— ç‰¹æ®Štoken
        )
        # å¼€å§‹è®­ç»ƒ
        tokenizer.train_from_iterator(text_iterator, trainer)
        return cls(tokenizer)

    def encode_ordinary(self, text):
        """å¿½ç•¥ä»»ä½•ç‰¹æ®Štokençš„ç¼–ç ã€‚"""
        ids = self.tokenizer.encode(text, add_special_tokens=False).ids
        return ids

# -----------------------------------------------------------------------------
# Test all of the above

@pytest.fixture(scope="module")
def enwik8_path():
    """ä¸‹è½½å¹¶ç¼“å­˜enwik8æ•°æ®é›†çš„fixtureã€‚"""
    import os
    import zipfile
    from nanochat.common import get_base_dir
    base_dir = get_base_dir()
    # download and unzip enwik8 to .cache directory
    enwik8_url = "https://mattmahoney.net/dc/enwik8.zip"
    enwik8_local_path = os.path.join(base_dir, "enwik8")
    enwik8_local_path_zip = os.path.join(base_dir, "enwik8.zip")
    if not os.path.exists(enwik8_local_path):
        print(f"Downloading enwik8 to {enwik8_local_path_zip}")
        import requests
        response = requests.get(enwik8_url)
        with open(enwik8_local_path_zip, "wb") as f:
            f.write(response.content)
        with zipfile.ZipFile(enwik8_local_path_zip, "r") as zip_ref:
            zip_ref.extractall(base_dir)
        print(f"Unzipped enwik8 to {enwik8_local_path}")
        os.remove(enwik8_local_path_zip)
        print(f"Removed {enwik8_local_path_zip}")
    else:
        print(f"Using existing enwik8 at {enwik8_local_path}")
    return enwik8_local_path


@pytest.fixture(scope="module")
def enwik8_small(enwik8_path):
    """æä¾›100KB enwik8ç”¨äºå¿«é€Ÿæµ‹è¯•çš„fixtureã€‚"""
    with open(enwik8_path, "r") as f:
        return f.read(100_000)

@pytest.fixture(scope="module")
def enwik8_large(enwik8_path):
    """æä¾›10MB enwik8ç”¨äºæ€§èƒ½æµ‹è¯•çš„fixtureã€‚"""
    with open(enwik8_path, "r") as f:
        return f.read(10**7)

def time_function(func, *args, **kwargs):
    """è®¡æ—¶å‡½æ•°è°ƒç”¨å¹¶è¿”å›ç»“æœå’Œç»è¿‡çš„æ—¶é—´"""
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    elapsed = end_time - start_time
    return result, elapsed

def test_correctness(enwik8_small):
    """æµ‹è¯•æ‰€æœ‰åˆ†è¯å™¨å®ç°äº§ç”Ÿç›¸åŒçš„ç»“æœã€‚"""
    text = enwik8_small
    encode_text = text
    vocab_size = 256 + 20  # 20 merges

    # Train slow reference
    print("\nTraining slow reference...")
    slow_reference_tokenizer = RegexTokenizer()
    ambiguous_flag, slow_reference_train_time = time_function(slow_reference_tokenizer.train, text, vocab_size)
    slow_reference_ids, slow_reference_encode_time = time_function(slow_reference_tokenizer.encode_ordinary, encode_text)
    print(f"Slow reference train time: {slow_reference_train_time:.4f}s")
    print(f"Slow reference encode time: {slow_reference_encode_time:.4f}s")
    print(slow_reference_ids[:20])

    if ambiguous_flag:
        print("â€¼ï¸ WARNING: merge order was detected to be ambiguous given current text and vocab size")
        print("The implementation could be correct but we might see different results below")
    else:
        print("âœ… Merge order is NOT ambiguous")

    # Train fast reference
    print("\nTraining fast reference...")
    fast_reference_tokenizer = FastRegexTokenizer()
    _, fast_reference_train_time = time_function(fast_reference_tokenizer.train, text, vocab_size)
    fast_reference_ids, fast_reference_encode_time = time_function(fast_reference_tokenizer.encode_ordinary, encode_text)
    print(f"Fast reference train time: {fast_reference_train_time:.4f}s")
    print(f"Fast reference encode time: {fast_reference_encode_time:.4f}s")
    print(fast_reference_ids[:20])

    # Assert fast equals slow
    assert fast_reference_ids == slow_reference_ids, "Fast reference should match slow reference"
    print("âœ… Fast == Slow")

    # Train HuggingFace
    print("\nTraining HuggingFace...")
    hf_tokenizer, hf_train_time = time_function(HuggingFaceTokenizer.train_from_iterator, [text], vocab_size)
    hf_ids, hf_encode_time = time_function(hf_tokenizer.encode_ordinary, encode_text)
    print(f"HuggingFace train time: {hf_train_time:.4f}s")
    print(f"HuggingFace encode time: {hf_encode_time:.4f}s")
    print(hf_ids[:20])

    # HuggingFaceæœ‰ä¸åŒçš„å­—èŠ‚é¡ºåºï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰åŒ¹é…
    def custom_match(ids1, ids2):
        """è‡ªå®šä¹‰åŒ¹é…å‡½æ•°ï¼Œå¤„ç†HuggingFaceçš„å­—èŠ‚é¡ºåºå·®å¼‚"""
        perm = {}
        for x, y in zip(ids1, ids2):
            if x < 256:
                if x in perm:
                    if perm[x] != y:
                        return False
                perm[x] = y
            if x >= 256 and x != y:
                return False
        return True

    assert custom_match(hf_ids, fast_reference_ids), "HuggingFace should match fast reference"
    print("âœ… HuggingFace == Fast")

    # Finally use our own Rust implementation
    print("\nTraining rustbpe...")
    rustbpe_tokenizer = rustbpe.Tokenizer()
    _, rustbpe_train_time = time_function(rustbpe_tokenizer.train_from_iterator, [text], vocab_size)
    rustbpe_ids, rustbpe_encode_time = time_function(rustbpe_tokenizer.encode, encode_text)
    print(f"RustBPE train time: {rustbpe_train_time:.4f}s")
    print(f"RustBPE encode time: {rustbpe_encode_time:.4f}s")
    print(rustbpe_ids[:20])

    assert rustbpe_ids == fast_reference_ids, "RustBPE should match fast reference"
    print("âœ… RustBPE == Fast")

    # Now export rustbpe to tiktoken for more efficient inference
    print("\nTesting tiktoken export...")
    pattern = rustbpe_tokenizer.get_pattern()
    mergeable_ranks_list = rustbpe_tokenizer.get_mergeable_ranks()
    mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}
    enc = tiktoken.Encoding(
        name="rustbpe",
        pat_str=pattern,
        mergeable_ranks=mergeable_ranks,
        special_tokens={},
    )
    tiktoken_ids, tiktoken_encode_time = time_function(enc.encode, encode_text)
    print(f"Tiktoken encode time: {tiktoken_encode_time:.4f}s")
    print(tiktoken_ids[:20])

    assert tiktoken_ids == rustbpe_ids, "Tiktoken should match RustBPE"
    print("âœ… Tiktoken == RustBPE")


@pytest.mark.slow
def test_training_performance(enwik8_large):
    """ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†æ¯”è¾ƒä¼˜åŒ–åˆ†è¯å™¨ï¼ˆPythonã€Rustã€HuggingFaceï¼‰çš„è®­ç»ƒé€Ÿåº¦ã€‚"""
    text = enwik8_large
    vocab_size = 2048
    print(f"\nText length: {len(text)}")

    # Commenting out because it's just way too slow to matter
    # Train optimized python version
    # print("Training optimized python version...")
    # optimized_python_tokenizer = FastRegexTokenizer()
    # _, optimized_python_train_time = time_function(optimized_python_tokenizer.train, text, vocab_size)
    # print(f"Optimized python train time: {optimized_python_train_time:.4f}s")

    # Train rustbpe
    print("\nTraining rustbpe...")
    rustbpe_tokenizer = rustbpe.Tokenizer()
    _, rustbpe_train_time = time_function(rustbpe_tokenizer.train_from_iterator, [text], vocab_size)
    print(f"RustBPE train time: {rustbpe_train_time:.4f}s")
    assert rustbpe_train_time > 0, "Training should take some time"

    # Train HuggingFace
    print("\nTraining HuggingFace...")
    hf_tokenizer, hf_train_time = time_function(HuggingFaceTokenizer.train_from_iterator, [text], vocab_size)
    print(f"HuggingFace train time: {hf_train_time:.4f}s")
    assert hf_train_time > 0, "Training should take some time"

    # Print comparison
    print(f"\nğŸ“Š Performance comparison:")
    print(f"   RustBPE: {rustbpe_train_time:.4f}s")
    print(f"   HuggingFace: {hf_train_time:.4f}s")
    print(f"   Speedup: {hf_train_time/rustbpe_train_time:.2f}x")

def test_interface(enwik8_small):
    """æµ‹è¯•RustBPETokenizeræ¥å£çš„è®­ç»ƒã€ç¼–ç ã€è§£ç å’Œåºåˆ—åŒ–åŠŸèƒ½ã€‚"""
    import tempfile
    from nanochat.tokenizer import RustBPETokenizer

    # Simple train test
    vocab_size = 300
    tok = RustBPETokenizer.train_from_iterator([enwik8_small], vocab_size)
    assert tok.get_vocab_size() == vocab_size, f"Expected vocab size {vocab_size}, got {tok.get_vocab_size()}"
    print(f"âœ… Trained tokenizer with vocab size {vocab_size}")

    # Encode/decode text
    encode_text = "Hello world! How are you? ğŸ™ƒ"
    ids = tok.encode(encode_text)
    print(f"\nInput text: {encode_text}")
    print(f"IDs: {ids}")
    decoded = tok.decode(ids)
    print(f"Decoded: {decoded}")
    assert decoded == encode_text, f"Decoded text doesn't match: {decoded} != {encode_text}"
    print("âœ… Encode/decode test passed")

    # Encode batch test
    ids_new = tok.encode([encode_text, encode_text])
    assert all(x == ids for x in ids_new), "Batch encoding should produce identical results"
    print("âœ… Encode batch OK")

    # append/prepend functionality
    ids_special = tok.encode(encode_text, prepend="<|bos|>", append="<|bos|>")
    bos_token_id = tok.encode_special("<|bos|>")
    assert ids_special == [bos_token_id] + ids + [bos_token_id], "Special tokens not correctly added"
    print("âœ… append/prepend OK")

    # Save/load test through a temporary directory
    with tempfile.TemporaryDirectory() as tmp_dir:
        tok.save(tmp_dir)
        tok_reloaded = RustBPETokenizer.from_directory(tmp_dir)
        ids_reloaded = tok_reloaded.encode(encode_text)
        assert ids_reloaded == ids, "Reloaded tokenizer should produce same results"
        print("âœ… Save/load through temporary directory OK")
