"""
ä½¿ç”¨HuggingFace Tokenizersåº“è®­ç»ƒä¸€ä¸ªåˆ†è¯å™¨ã€‚
é‡‡ç”¨GPT-4åˆ†è¯å™¨çš„é£æ ¼ã€‚
"""
import os
import time
import argparse
import torch
from nanochat.tokenizer import RustBPETokenizer
from nanochat.common import get_base_dir
from nanochat.dataset import parquets_iter_batched

# -----------------------------------------------------------------------------
# è§£æå‘½ä»¤è¡Œå‚æ•°

parser = argparse.ArgumentParser(description='è®­ç»ƒä¸€ä¸ªBPEåˆ†è¯å™¨')
parser.add_argument('--max_chars', type=int, default=10_000_000_000, help='è®­ç»ƒçš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼š100äº¿ï¼‰')
parser.add_argument('--doc_cap', type=int, default=10_000, help='æ¯ä¸ªæ–‡æ¡£çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼š10,000ï¼‰')
parser.add_argument('--vocab_size', type=int, default=65536, help='è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤ï¼š65536 = 2^16ï¼‰')
args = parser.parse_args()
print(f"æœ€å¤§å­—ç¬¦æ•°: {args.max_chars:,}")
print(f"æ–‡æ¡£ä¸Šé™: {args.doc_cap:,}")
print(f"è¯æ±‡è¡¨å¤§å°: {args.vocab_size:,}")

# -----------------------------------------------------------------------------
# æ–‡æœ¬è¿­ä»£å™¨

def text_iterator():
    """
    1) å°†æ‰¹æ¬¡å±•å¹³ä¸ºå•ä¸ªè¿­ä»£å™¨
    2) å°†æ¯ä¸ªæ–‡æ¡£è£å‰ªåˆ°args.doc_capä¸ªå­—ç¬¦
    3) å½“æˆ‘ä»¬çœ‹åˆ°args.max_charsä¸ªå­—ç¬¦æ—¶ä¸­æ–­
    """
    nchars = 0
    for batch in parquets_iter_batched(split="train"):
        for doc in batch:
            doc_text = doc
            if len(doc_text) > args.doc_cap:
                doc_text = doc_text[:args.doc_cap]
            nchars += len(doc_text)
            yield doc_text
            if nchars > args.max_chars:
                return
text_iter = text_iterator()

# -----------------------------------------------------------------------------
# è®­ç»ƒåˆ†è¯å™¨
t0 = time.time()
tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)
t1 = time.time()
train_time = t1 - t0
print(f"è®­ç»ƒæ—¶é—´: {train_time:.2f}s")

# -----------------------------------------------------------------------------
# å°†åˆ†è¯å™¨ä¿å­˜åˆ°ç£ç›˜
base_dir = get_base_dir()
tokenizer_dir = os.path.join(base_dir, "tokenizer")
tokenizer.save(tokenizer_dir)

# -----------------------------------------------------------------------------
# å¿«é€Ÿå†…è”å®Œæ•´æ€§æ£€æŸ¥
test_text = """Hello world! This is a test.
Numbers: 123, 4567, 89
Contractions: I'm, you're, it's
Special chars: @#$%^&*()
Unicode: ä½ å¥½ä¸–ç•Œ ğŸŒ"""
encoded = tokenizer.encode(test_text)
decoded = tokenizer.decode(encoded)
assert decoded == test_text

# -----------------------------------------------------------------------------
# è¿˜æœ‰ä¸€ä»¶äº‹ï¼šæˆ‘ä»¬å¸Œæœ›ç¼“å­˜ä»token idåˆ°è¯¥tokenå­—èŠ‚æ•°çš„æ˜ å°„
# ä»¥ä¾¿é«˜æ•ˆè¯„ä¼°æ¯å­—èŠ‚ä½æ•°ã€‚ä¸å…¸å‹çš„å¹³å‡æŸå¤±ä¸åŒï¼Œè¿™
# å…è®¸æˆ‘ä»¬æŠ¥å‘Šä¸€ä¸ªä¸éšåˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°å˜åŒ–çš„æŸå¤±ã€‚
# éªŒè¯é›†ä¸Šçš„æ¯å­—èŠ‚ä½æ•°æ˜¯æˆ‘ä»¬å…³å¿ƒçš„ä¸»è¦æŒ‡æ ‡ä¹‹ä¸€ã€‚
vocab_size = tokenizer.get_vocab_size()
special_set = set(tokenizer.get_special_tokens())
token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]
token_bytes = []
for token_id in range(vocab_size):
    token_str = token_strings[token_id] # æ­¤tokençš„Pythonå­—ç¬¦ä¸²è¡¨ç¤º
    if token_str in special_set:
        token_bytes.append(0) # ç‰¹æ®Šå­—ç¬¦ä¸è®¡å…¥
    else:
        id_bytes = len(token_str.encode("utf-8")) # æ„æˆæ­¤tokençš„å­—èŠ‚æ•°
        token_bytes.append(id_bytes)
token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')
token_bytes_path = os.path.join(tokenizer_dir, "token_bytes.pt")
with open(token_bytes_path, "wb") as f:
    torch.save(token_bytes, f)
print(f"å·²ä¿å­˜token_bytesåˆ° {token_bytes_path}")

# è®°å½•åˆ°æŠ¥å‘Š
from nanochat.report import get_report
token_bytes_nonzero = (token_bytes[token_bytes > 0]).to(dtype=torch.float32)
get_report().log(section="åˆ†è¯å™¨è®­ç»ƒ", data=[
    vars(args), # argparseå‘½ä»¤è¡Œå‚æ•°
    {"è®­ç»ƒæ—¶é—´": train_time},
    {"ç‰¹æ®Štokenæ•°é‡": len(special_set)},
    {
        "tokenå­—èŠ‚æ•°æœ€å°å€¼": int(token_bytes_nonzero.min().item()),
        "tokenå­—èŠ‚æ•°æœ€å¤§å€¼": int(token_bytes_nonzero.max().item()),
        "tokenå­—èŠ‚æ•°å¹³å‡å€¼": token_bytes_nonzero.mean().item(),
        "tokenå­—èŠ‚æ•°æ ‡å‡†å·®": token_bytes_nonzero.std().item(),
    }
])
